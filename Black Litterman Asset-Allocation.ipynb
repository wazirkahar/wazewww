{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import sys\n",
    "sys.path.append('C:/Users/Wazir')\n",
    "import edhec_risk_kit as erk\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Black Litterman Formulas\n",
    "\n",
    "Assume that we have $N$ assets, and $K$ views. There are two sets of inputs to the procedure. The first set of inputs relate to market parameters and these are:\n",
    "\n",
    "\\begin{array}{ll}\n",
    "w & \\mbox{A Column Vector ($N \\times 1$) of Equilibrium Market Weights of the Assets} \\\\\n",
    "\\Sigma & \\mbox{A Covariance Matrix ($N \\times N$) of the Assets} \\\\\n",
    "R_f & \\mbox{The Risk Free Rate} \\\\\n",
    "\\delta & \\mbox{The investor's Risk Aversion parameter}  \\\\\n",
    "\\tau & \\mbox{A scalar indicating the uncertainty of the prior (details below)}\n",
    "\\end{array}\n",
    "\n",
    "\n",
    "Some of these parameters can be inferred from other parameters if they are not explicitly specified. For instance, the risk aversion parameter can be set arbitrarily. For instance, some authors use $\\delta = 2.5$ while others use the value of $\\delta = 2.14$ in order to be consistent with the value calculated in \\cite{dimson2008triumph}.\n",
    "\n",
    "\\cite{beach2007application} suggest using $2.65$. Another common approach is to set $\\delta$ to the Market Price of Risk (i.e. a measure of the risk aversion of the Representative Investor, which is computed as $\\delta = \\mu_M/\\sigma^2_M$ where $\\mu_M$ and $\\sigma^2_M$ are estimates of the mean and variance of the returns of the market portfolio. Frequently, a broad market index such as the S\\&P500 is taken as a proxy for the market in order to compute the market price of risk from $\\mu_M$ and $\\sigma^2_M$.\n",
    "\n",
    "The treatment of $\\tau$ is the source of some confusion. As we will explain in the following section, some implementors have done away with $\\tau$ by setting it to $1$ or to calibrate the model to $tau$. In the original model, Black and Litterman suggest using a small number. A common technique is to set $\\tau = 1/T$ where $T$ is the number of periods of data used. Thus, for $T=5$ you would use $1/(5 \\times 12)$ which yields a value of approximately $\\tau=.02$.\n",
    "\n",
    "The second set of inputs that the procedure needs is a representation of the investors views. These are specified via:\n",
    "\n",
    "\\begin{array}{ll}\n",
    "Q & \\mbox{An $K \\times 1$ ``Qualitative Views'' or simply, Views matrix} \\\\\n",
    "P & \\mbox{A $K \\times N$ ``Projection'' or ``Pick'' matrix, linking each view to the assets} \\\\\n",
    "\\Omega & \\mbox{A Covariance matrix representing the uncertainty of views}\n",
    "\\end{array}\n",
    "\n",
    "\n",
    "Views are represented in $Q$ and $P$ as follows:\n",
    "\n",
    "If the $k$-th view is an absolute view, it is represented by setting $Q_k$ to the expected return of asset $k$ and setting $P_{ki}$ to 1 and all other elements of row $k$ in $P$ to zero.\n",
    "\n",
    "If the $k$-th view is an relative view, between assets $i$ and $j$ it is represented by setting $Q_k$ to the expected difference of returns between assets $i$ and $j$, and setting $P_{ki}$ to $-1$ for the underperforming asset, $P_{kj}$ to $+1$ and all other elements of row $k$ in $P$ to zero. $\\Omega$ is either set to the specified uncertainty or is inferred from the user or from the data.\n",
    "\n",
    "The uncertainty of the views $\\Omega$ is either set by the user, or inferred (e.g. via statements of confidence, from market data, from the variance of residuals from a prediction model used to generate the views etc, we shall see examples in sections below). In particular, \\cite{he1999intuition} suggest setting it to be the diagonal matrix obtained from the diagonal elements of $P \\tau \\Sigma P^T$, which is what we shall do for some of our initial tests. In my implementation the code accepts a matrix, but uses this assumption as the default if the user does not specify a matrix to use as $\\Omega$.\n",
    "\n",
    "#### The Master Formula\n",
    "\n",
    "The first step of the procedure is a _reverse-optimization_ step that infers the implied returns vector $\\pi$ that are implied by the equilibrium weights $w$ using the formula:\n",
    "\n",
    "$$\\pi = \\delta\\Sigma w$$\n",
    "\n",
    "Next, the posterior returns and covariances are obtained from the _Black-Litterman Master Formula_ which is the following set of equations:\n",
    "\n",
    "\\begin{equation}\n",
    "\\label{eq:blMuOrig}\n",
    "\\mu^{BL} = [(\\tau\\Sigma)^{-1} + P \\Omega^{-1} P]^{-1}[(\\tau\\Sigma)^{-1} \\pi + P \\Omega^{-1} Q]\n",
    "\\end{equation}\n",
    "\n",
    "\\begin{equation}\n",
    "\\label{eq:blSigmaOrig}\n",
    "\\Sigma^{BL} = \\Sigma + [(\\tau\\Sigma)^{-1} + P \\Omega^{-1} P]^{-1}\n",
    "\\end{equation}\n",
    "\n",
    "#### Inverting $\\Omega$\n",
    "\n",
    "While the master formulas identified in Equation \\ref{eq:blMuOrig} and Equation \\ref{eq:blSigmaOrig} are frequently easy to implement, they do involve the term $\\Omega^{-1}$. Unfortuantely, $\\Omega$ is sometimes non-invertible, which poses difficulties to implement the equations as-is. Fortunately the equations are easily transformed to a form that does not require this troublesome inversion. Therefore, frequently, implementations use the following equivalent versions of these equations which are sometimes computationally more stable, since they do not involve inverting $\\Omega$. Derivations of these alternate forms are provided in the appendices of \\cite{walters2011black}:\n",
    "\n",
    "\\begin{equation}\n",
    "\\label{eq:blMu}\n",
    "\\mu^{BL} = \\pi + \\tau \\Sigma P^T[(P \\tau \\Sigma P^T) + \\Omega]^{-1}[Q - P \\pi]\n",
    "\\end{equation}\n",
    "\n",
    "\\begin{equation}\n",
    "\\label{eq:blSigma}\n",
    "\\Sigma^{BL} = \\Sigma + \\tau \\Sigma - \\tau\\Sigma P^T(P \\tau \\Sigma P^T + \\Omega)^{-1} P \\tau \\Sigma\n",
    "\\end{equation}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ascolvec(x):\n",
    "    if (x.ndim ==2):\n",
    "        return x\n",
    "    else:\n",
    "        return np.expand_dims(x, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = np.array([[1,2,3],[1,3,4]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 2, 3],\n",
       "       [1, 3, 4]])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ascolvec(a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recall that the first step in the Black Litterman procedure was to reverse engineer the implied returns vector $\\pi$ from a set of portfolio weights $w$. \n",
    "\n",
    "$$\\pi = \\delta\\Sigma w$$\n",
    "\n",
    "This is performed by the following code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def implied_returns(delta, sigma, w):\n",
    "    \"\"\"\n",
    "Obtain the implied expected returns by reverse engineering the weights\n",
    "Inputs:\n",
    "delta: Risk Aversion Coefficient (scalar)\n",
    "sigma: Variance-Covariance Matrix (N x N) as DataFrame\n",
    "    w: Portfolio weights (N x 1) as Series\n",
    "Returns an N x 1 vector of Returns as Series\n",
    "    \"\"\"\n",
    "    ir = delta * sigma.dot(w).squeeze() # to get a series from a 1-column dataframe\n",
    "    ir.name = 'Implied Returns'\n",
    "    return ir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 2, 3],\n",
       "       [1, 3, 4]])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.squeeze()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we noted previously, \\cite{he1999intuition} suggest that if the investor does not have a specific way to explicitly quantify the uncertaintly associated with the view in the $\\Omega$ matrix, one could make the simplifying assumption that $\\Omega$ is proportional to the variance of the prior.\n",
    "\n",
    "Specifically, they suggest that:\n",
    "\n",
    "$$\\Omega = diag(P (\\tau \\Sigma) P^T) $$\n",
    "\n",
    "This is implemented in Python as:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def proportional_prior(sigma, tau, p):\n",
    "    \"\"\"\n",
    "    Returns the He-Litterman simplified Omega\n",
    "    Inputs:\n",
    "    sigma: N x N Covariance Matrix as DataFrame\n",
    "    tau: a scalar\n",
    "    p: a K x N DataFrame linking Q and Assets\n",
    "    returns a P x P DataFrame, a Matrix representing Prior Uncertainties\n",
    "    \"\"\"\n",
    "    \n",
    "    helit_omega = p.dot(tau*sigma).dot(p.T)\n",
    "    return pd.DataFrame(np.diag(np.diag(helit_omega.values)),index=p.index, columns=p.index)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use this function to compute the posterior expected returns as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bl(w_prior, sigma_prior, p, q, omega=None, delta=2.5, tau=0.2):\n",
    "    \"\"\"\n",
    "    Computes the posterior expected returns based on \n",
    "    the original black litterman reference model\n",
    "    W.prior must be an N x 1 vector of weights, a Series\n",
    "    Sigma.prior is an N x N covariance matrix, a DataFrame\n",
    "    P must be a K x N matrix linking Q and the Assets, a DataFrame\n",
    "    Q must be an K x 1 vector of views, a Series\n",
    "    Omega must be a K x K matrix a DataFrame, or None\n",
    "    if Omega is None, we assume it is\n",
    "    proportional to variance of the prior\n",
    "    delta and tau are scalars\n",
    "    \"\"\"\n",
    "    if omega is None:\n",
    "        omega = proportional_prior(sigma_prior, tau, p)\n",
    "    #Number of Assets\n",
    "    N = w_prior.shape[0]\n",
    "    #Number of views\n",
    "    K = q.shape[0]\n",
    "    #Reverse engineer the weights to get pi\n",
    "    pi = implied_returns(delta, sigma_prior, w_prior)\n",
    "    #Adjust sigma by the uncertainty factor\n",
    "    sigma_prior_scaled = sigma_prior*tau\n",
    "    #Posterior estimate of the mean\n",
    "    mu_bl = pi + sigma_prior_scaled.dot(p.T).dot(inv(p.dot(sigma_prior_scaled).dot(p.T)+omega).dot(q-p.dot(pi).values))\n",
    "    #Posterior estimate for uncertainty of the mean\n",
    "    sigma_bl = sigma_prior + sigma_prior_scaled - sigma_prior_scaled.dot(p.T).dot(inv(p.dot(sigma_prior_scaled).dot(p.T) + omega)).dot(p).dot(sigma_prior_scaled)\n",
    "    return (mu_bl, sigma_bl)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A Simple Example: Absolute Views\n",
    "\n",
    "We start with a simple 2-Asset example. Let's start with an example from _Statistical Models and Methods for Financial Markets (Springer Texts in Statistics) 2008th Edition, Tze Lai and Haipeng Xing_.\n",
    "\n",
    "Consider the portfolio consisting of just two stocks: Intel (INTC) and Pfizer (PFE).\n",
    "\n",
    "From Table 3.1 on page 72 of the book, we obtain the covariance matrix (multipled by $10^4$)\n",
    "\n",
    "\\begin{array}{lcc}\n",
    "INTC & 46.0 & 1.06 \\\\\n",
    "PFE   & 1.06 & 5.33\n",
    "\\end{array}\n",
    "\n",
    "Assume that Intel has a market capitalization of approximately USD 80B and that of Pfizer is approximately USD 100B (this is not quite accurate, but works just fine as an example!).\n",
    "Thus, if you held a market-cap weighted portfolio you would hold INTC and PFE with the following weights: $W_{INTC} = 80/180 = 44\\%, W_{PFE} = 100/180 = 56\\%$. These appear to be reasonable weights without an extreme allocation to either stock, even though Pfizer is slightly overweighted.\n",
    "\n",
    "We can compute the equilibrium implied returns $\\pi$ as follows:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>INTC</th>\n",
       "      <th>PFE</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>INTC</th>\n",
       "      <td>0.04600</td>\n",
       "      <td>0.00106</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PFE</th>\n",
       "      <td>0.00106</td>\n",
       "      <td>0.00533</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         INTC      PFE\n",
       "INTC  0.04600  0.00106\n",
       "PFE   0.00106  0.00533"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cov = pd.DataFrame([[46.0, 1.06], [1.06,5.33]], index=['INTC', 'PFE'], columns = ['INTC', 'PFE']) *10E-4\n",
    "cov"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "INTC    0.052084\n",
       "PFE     0.008628\n",
       "Name: Implied Returns, dtype: float64"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pi = implied_returns(delta= 2.5, sigma=cov, w=pd.Series([.44, .56], index=['INTC', 'PFE']))\n",
    "pi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thus the equilibrium implied returns for INTC are a bit more than 5\\% and a bit less than 1\\% for PFE.\n",
    "\n",
    "Assume that the investor thinks that Intel will return 2\\% and that Pfizer is poised to rebounce, and will return 4\\% . We can now examine the optimal weights according to the Markowitz procedure.\n",
    "What would happen if we used these expected returns to compute the Optimal Max Sharpe Ratio portfolio?\n",
    "\n",
    "The Max Sharpe Ratio (MSR) Portfolio weights are easily computed in explicit form if there are no constraints on the weights.\n",
    "The weights are given by the expression (e.g. See  \\cite{campbell1996econometrics} page 188 Equation 5.2.28):\n",
    "\n",
    "$$ W_{MSR} = \\frac{\\Sigma^{-1}\\mu_e}{\\bf{1}^T \\Sigma^{-1}\\mu_e} $$\n",
    "\n",
    "where $\\mu_e$ is the vector of expected excess returns and $\\Sigma$ is the variance-covariance matrix.\n",
    "\n",
    "This is implemented as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inverse(d):\n",
    "    '''\n",
    "    Invert a dataframe values\n",
    "    '''\n",
    "    return pd.DataFrame(inv(d.values), index=d.index, columns=d.columns)\n",
    "\n",
    "def w_msr(sigma, mu, scale=True):\n",
    "    \"\"\"\n",
    "    Optimal (Tangent/Max Sharpe Ratio) Portfolio weights\n",
    "    by using the Markowitz Optimization Procedure\n",
    "    Mu is the vector of Excess expected Returns\n",
    "    Sigma must be an N x N matrix as a DataFrame and Mu a column vector as a Series\n",
    "    This implements page 188 Equation 5.2.28 of\n",
    "    \"The econometrics of financial markets\" Campbell, Lo and Mackinlay.\n",
    "    \"\"\"\n",
    "    w = inverse(sigma).dot(mu)\n",
    "    if scale:\n",
    "        w = w/sum(w) # fix: this assumes all w is +ve\n",
    "    return w\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recall that the investor expects that Intel will return 2\\% and Pfizer will return 4\\% . We can now examine the optimal weights obtained by naively implementing the Markowitz procedure with these expected returns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "INTC     3.41\n",
       "PFE     96.59\n",
       "dtype: float64"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from numpy.linalg import inv\n",
    "mu_exp = pd.Series([.02, .04],index=['INTC', 'PFE']) # INTC and PFE\n",
    "np.round(w_msr(cov, mu_exp)*100, 2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consistent with the poor reputation of naive Markowitz optimization, the Markwitz procedure places an unrealistic weight of more than 96\\% in Pfizer and less than 4\\% in Intel. This is completely impractical and no reasonable investor would make such dramatic bets.\n",
    "\n",
    "In contrast, let us now find the weights that the Black Litterman procedure would place. We allow $\\Omega$ to be computed automatically, and are willing to use all the other defaults. We find the Black Litterman weights as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Absolute view 1: INTC will return 2%\n",
    "# Absolute view 2: PFE will return 4%\n",
    "q= pd.Series({'INTC': 0.02, 'PFE': 0.04})\n",
    "\n",
    "p = pd.DataFrame([{'INTC': 1, 'PFE': 0}, \n",
    "                  {'INTC' :0, 'PFE': 1}])\n",
    "\n",
    "bl_mu, bl_sigma = bl(w_prior=pd.Series({'INTC': 0.44, 'PFE': 0.56}), sigma_prior=cov, p=p, q=q)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "INTC    0.037622\n",
       "PFE     0.024111\n",
       "dtype: float64"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bl_mu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>INTC</th>\n",
       "      <th>PFE</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>INTC</th>\n",
       "      <td>0.050595</td>\n",
       "      <td>0.001113</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PFE</th>\n",
       "      <td>0.001113</td>\n",
       "      <td>0.005862</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          INTC       PFE\n",
       "INTC  0.050595  0.001113\n",
       "PFE   0.001113  0.005862"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bl_sigma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "INTC    0.141221\n",
       "PFE     0.858779\n",
       "dtype: float64"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Use the Black Litterman expected returns to get the Optimal Markowitz weights\n",
    "w_msr(bl_sigma, bl_mu)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that we get much more reasonable weights than we did with naive optimization. These weights are also much closer to the 45-55 mix in the cap weighted portfolio.\n",
    "On the other hand, they respect the investor's view that expects Pfizer to rebound, and places a higher weight on Pfizer relative to the cap weighted portfolio.\n",
    "\n",
    "### A Simple Example: Relative Views\n",
    "\n",
    "In this example, we examine relative views. We stick with our simple 2-stock example. Recall that the Cap-Weighted implied expected returns are:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "INTC    0.052084\n",
       "PFE     0.008628\n",
       "Name: Implied Returns, dtype: float64"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recall also that the cap-weighted portfolio is approximately a 45-55 mix of Intel and Pfizer.\n",
    "\n",
    "Assume instead that the investor feels that the Intel will outperform Pfizer by only 2\\%. This view is implemented as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "q= pd.Series([0.02])\n",
    "\n",
    "p = pd.DataFrame([{'INTC': 1, 'PFE': -1}])\n",
    "\n",
    "bl_mu, bl_sigma = bl(w_prior=pd.Series({'INTC': 0.44, 'PFE': 0.56}), sigma_prior=cov, p=p, q=q)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "INTC    0.041374\n",
       "PFE     0.009646\n",
       "dtype: float64"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bl_mu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once again we see that the Black Litterman expected returns are a blend between the cap-weight implied weights and the investor view. The outperformance of Intel in the implied returns is:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.043456"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pi[0] - pi[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In contrast, the investor felt it only would be 2\\%. The expected returns returned by the Black Litterman procedure show a spread that is a blend between the cap-weight implied returns and that of the investor:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.031728"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bl_mu[0] - bl_mu[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "INTC    0.368115\n",
       "PFE     0.631885\n",
       "dtype: float64"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Use the Black Litterman expected returns and covariance matrix\n",
    "w_msr(bl_sigma, bl_mu)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These seem like reasonable weights, and demonstrates the power of using the Black Litterman procedure. In contrast, consider the weights we would get if we implemented the same view without Black Litterman. We set the returns of Intel and Pfizer to be 3\\% and 1\\% respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "INTC    0.258528\n",
       "PFE     0.741472\n",
       "dtype: float64"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w_msr(cov, [.03, .01])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The weights are significantly more dramatic than one might be willing to implement, and are likely unwarranted given the relatively weak view. In fact, if the same view were implemented as Intel and Pfizer returning 2\\% and 0\\%, the results are even more extreme:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "INTC    1.248244\n",
       "PFE    -0.248244\n",
       "dtype: float64"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w_msr(cov, [.02, .0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this case, the Markowitz recommends shorting Pfizer to the extent of nearly 25\\% of the portfolio and leveraging Intel to 125\\%. Clearly this is not a plausible allocation based on the simple view expressed above.\n",
    "\n",
    "## Reproducing the He-Litterman (1999) Results\n",
    "\n",
    "We now reproduce the results in the He-Litterman paper that first detailed the steps in the procedure. We obtained the data by typing it in from the He-Litterman tables, and used it to test the implementation.\n",
    "\n",
    "The He-Litterman example involves an international allocation between 7 countries. The data is as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AU    3.9\n",
       "CA    6.9\n",
       "FR    8.4\n",
       "DE    9.0\n",
       "JP    4.3\n",
       "UK    6.8\n",
       "US    7.6\n",
       "Name: Implied Returns, dtype: float64"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The 7 countries ...\n",
    "countries  = ['AU', 'CA', 'FR', 'DE', 'JP', 'UK', 'US'] \n",
    "# Table 1 of the He-Litterman paper\n",
    "# Correlation Matrix\n",
    "rho = pd.DataFrame([\n",
    "    [1.000,0.488,0.478,0.515,0.439,0.512,0.491],\n",
    "    [0.488,1.000,0.664,0.655,0.310,0.608,0.779],\n",
    "    [0.478,0.664,1.000,0.861,0.355,0.783,0.668],\n",
    "    [0.515,0.655,0.861,1.000,0.354,0.777,0.653],\n",
    "    [0.439,0.310,0.355,0.354,1.000,0.405,0.306],\n",
    "    [0.512,0.608,0.783,0.777,0.405,1.000,0.652],\n",
    "    [0.491,0.779,0.668,0.653,0.306,0.652,1.000]], index=countries, columns=countries)\n",
    "\n",
    "# Table 2 of the He-Litterman paper: volatilities\n",
    "vols = pd.DataFrame([0.160,0.203,0.248,0.271,0.210,0.200,0.187],index=countries, columns=[\"vol\"]) \n",
    "# Table 2 of the He-Litterman paper: cap-weights\n",
    "w_eq = pd.DataFrame([0.016,0.022,0.052,0.055,0.116,0.124,0.615], index=countries, columns=[\"CapWeight\"])\n",
    "# Compute the Covariance Matrix\n",
    "sigma_prior = vols.dot(vols.T) * rho\n",
    "# Compute Pi and compare:\n",
    "pi = implied_returns(delta=2.5, sigma=sigma_prior, w=w_eq)\n",
    "(pi*100).round(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### View 1: Germany vs Rest of Europe\n",
    "\n",
    "Next, we impose the view that German equities will outperform the rest of European equities by 5\\%.\n",
    "\n",
    "The other European equities are France and the UK. We split the outperformance proportional to the Market Caps of France and the UK.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>AU</th>\n",
       "      <th>CA</th>\n",
       "      <th>FR</th>\n",
       "      <th>DE</th>\n",
       "      <th>JP</th>\n",
       "      <th>UK</th>\n",
       "      <th>US</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-29.5</td>\n",
       "      <td>100.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-70.5</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    AU   CA    FR     DE   JP    UK   US\n",
       "0  0.0  0.0 -29.5  100.0  0.0 -70.5  0.0"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Germany will outperform other European Equities (i.e. FR and UK) by 5%\n",
    "q = pd.Series([.05]) # just one view\n",
    "# start with a single view, all zeros and overwrite the specific view\n",
    "p = pd.DataFrame([0.]*len(countries), index=countries).T\n",
    "# find the relative market caps of FR and UK to split the\n",
    "# relative outperformance of DE ...\n",
    "w_fr =  w_eq.loc[\"FR\"]/(w_eq.loc[\"FR\"]+w_eq.loc[\"UK\"])\n",
    "w_uk =  w_eq.loc[\"UK\"]/(w_eq.loc[\"FR\"]+w_eq.loc[\"UK\"])\n",
    "p.iloc[0]['DE'] = 1.\n",
    "p.iloc[0]['FR'] = -w_fr\n",
    "p.iloc[0]['UK'] = -w_uk\n",
    "(p*100).round(1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " The results of implementing this view appear in the He-Litterman paper in Table 4. This exactly reproduces column 1 of Table 4. Next, we examine the values of $\\mu^{BL}$:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AU     4.3\n",
       "CA     7.6\n",
       "FR     9.3\n",
       "DE    11.0\n",
       "JP     4.5\n",
       "UK     7.0\n",
       "US     8.1\n",
       "dtype: float64"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "delta = 2.5\n",
    "tau = 0.05 # from Footnote 8\n",
    "# Find the Black Litterman Expected Returns\n",
    "bl_mu, bl_sigma = bl(w_eq, sigma_prior, p, q, tau = tau)\n",
    "(bl_mu*100).round(1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The  Black Litterman expected returns computed by the code exactly reproduces column 2 of Table 4.\n",
    "\n",
    "He-Litterman compute the optimal portfolio $w^*$ as follows (this is Equation (13) on page 6 of their paper)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AU     1.5\n",
       "CA     2.1\n",
       "FR    -4.0\n",
       "DE    35.4\n",
       "JP    11.0\n",
       "UK    -9.5\n",
       "US    58.6\n",
       "dtype: float64"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def w_star(delta, sigma, mu):\n",
    "    return (inverse(sigma).dot(mu))/delta\n",
    "\n",
    "wstar = w_star(delta=2.5, sigma=bl_sigma, mu=bl_mu)\n",
    "# display w*\n",
    "(wstar*100).round(1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The computed $w^*$ exactly replicates column 3 ($w^*$) of Table 4. Finally, they compute $w^* - \\frac{w_{eq}}{1+\\tau}$ which is the difference in weights between the optimal portfolio and the equilibrium portfolio (they use unscaled weights) in column 4. We replicate that column as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AU     1.6\n",
       "CA     2.2\n",
       "FR     5.2\n",
       "DE     5.5\n",
       "JP    11.6\n",
       "UK    12.4\n",
       "US    61.5\n",
       "dtype: float64"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "delta = 2.5\n",
    "tau = 0.05\n",
    "w_eq  = w_msr(delta*sigma_prior, pi, scale=False)\n",
    "# Display the difference in Posterior and Prior weights\n",
    "w_eq*100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AU     0.0\n",
       "CA    -0.0\n",
       "FR    -8.9\n",
       "DE    30.2\n",
       "JP     0.0\n",
       "UK   -21.3\n",
       "US    -0.0\n",
       "dtype: float64"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.round(wstar - w_eq/(1+tau), 3)*100\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "which exactly matches Column 4 of Table 4. This completes our reproduction of the first view in He-Litterman (1999).\n",
    "\n",
    "Note that this demonstrates the power of the approach. The weights for assets that do not involve the view remain unchanged. The two underperforming countries (according to the view) are underweighted, while the overperforming country is overweighted, but not to the extreme extent that a naive portfolio optimizer would have produced."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### View 2: Canada vs US\n",
    "\n",
    "For their second case, He and Litterman implement the additional view that Canadian Equities will outperform US Equities by 3\\%. The results are in (their) Table 5, which we shall now reproduce."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    0.05\n",
       "dtype: float64"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>AU</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CA</th>\n",
       "      <td>0.0</td>\n",
       "      <td>100.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>FR</th>\n",
       "      <td>-29.5</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DE</th>\n",
       "      <td>100.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>JP</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>UK</th>\n",
       "      <td>-70.5</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>US</th>\n",
       "      <td>0.0</td>\n",
       "      <td>-100.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        0      1\n",
       "AU    0.0    0.0\n",
       "CA    0.0  100.0\n",
       "FR  -29.5    0.0\n",
       "DE  100.0    0.0\n",
       "JP    0.0    0.0\n",
       "UK  -70.5    0.0\n",
       "US    0.0 -100.0"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "view2 = pd.Series([.03], index=[1])\n",
    "q = q.append(view2)\n",
    "pick2 = pd.DataFrame([0.]*len(countries), index=countries, columns=[1]).T\n",
    "p = p.append(pick2) \n",
    "p.iloc[1]['CA']=+1\n",
    "p.iloc[1]['US']=-1\n",
    "np.round(p.T, 3)*100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AU     4.4\n",
       "CA     8.7\n",
       "FR     9.5\n",
       "DE    11.2\n",
       "JP     4.6\n",
       "UK     7.0\n",
       "US     7.5\n",
       "dtype: float64"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bl_mu, bl_sigma = bl(w_eq, sigma_prior, p, q, tau = tau)\n",
    "np.round(bl_mu*100, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AU     1.5\n",
       "CA    41.9\n",
       "FR    -3.4\n",
       "DE    33.6\n",
       "JP    11.0\n",
       "UK    -8.2\n",
       "US    18.8\n",
       "dtype: float64"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wstar = w_star(delta=2.5, sigma=bl_sigma, mu=bl_mu)\n",
    "# display w*\n",
    "(wstar*100).round(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AU     1.6\n",
       "CA     2.2\n",
       "FR     5.2\n",
       "DE     5.5\n",
       "JP    11.6\n",
       "UK    12.4\n",
       "US    61.5\n",
       "dtype: float64"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w_eq  = w_msr(delta*sigma_prior, pi, scale=False)\n",
    "w_eq*100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AU    -0.0\n",
       "CA    39.8\n",
       "FR    -8.4\n",
       "DE    28.4\n",
       "JP    -0.0\n",
       "UK   -20.0\n",
       "US   -39.8\n",
       "dtype: float64"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Display the difference in Posterior and Prior weights\n",
    "np.round(wstar - w_eq/(1+tau), 3)*100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Which exactly reproduces the last column of Table 5 of their paper.\n",
    "\n",
    "Once again, we see the power of the approach. The weights for assets that do not involve the view (AU, JP) remain unchanged. The two underperforming countries (FR, UK, US, according to the view) are underweighted, while the overperforming countries (CA, DE) are overweighted, but not to the extreme extent that a naive portfolio optimizer would have produced.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### View 4: Increasing View Uncertainty\n",
    "\n",
    "As a final step, He and Litterman demonstrate the effect of $\\Omega$. They increase the uncertainty associated with the first of the two views (i.e. the one that Germany will outperform the rest of Europe). First we compute the default value of $\\Omega$ and then increase the uncertainty associated with the first view alone."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>AU</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CA</th>\n",
       "      <td>0.0</td>\n",
       "      <td>100.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>FR</th>\n",
       "      <td>-29.5</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DE</th>\n",
       "      <td>100.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>JP</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>UK</th>\n",
       "      <td>-70.5</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>US</th>\n",
       "      <td>0.0</td>\n",
       "      <td>-100.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        0      1\n",
       "AU    0.0    0.0\n",
       "CA    0.0  100.0\n",
       "FR  -29.5    0.0\n",
       "DE  100.0    0.0\n",
       "JP    0.0    0.0\n",
       "UK  -70.5    0.0\n",
       "US    0.0 -100.0"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# This is the default \"Proportional to Prior\" assumption\n",
    "omega = proportional_prior(sigma_prior, tau, p)\n",
    "# Now, double the uncertainty associated with View 1\n",
    "omega.iloc[0,0] = 2*omega.iloc[0,0]\n",
    "np.round(p.T*100, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AU     4.3\n",
       "CA     8.5\n",
       "FR     9.2\n",
       "DE    10.6\n",
       "JP     4.6\n",
       "UK     6.9\n",
       "US     7.3\n",
       "dtype: float64"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bl_mu, bl_sigma = bl(w_eq, sigma_prior, p, q, tau = tau, omega=omega)\n",
    "np.round(bl_mu, 3)*100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AU     1.5\n",
       "CA    42.4\n",
       "FR    -0.6\n",
       "DE    24.0\n",
       "JP    11.0\n",
       "UK    -1.4\n",
       "US    18.3\n",
       "dtype: float64"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wstar = w_star(delta=2.5, sigma=bl_sigma, mu=bl_mu)\n",
    "# display w*\n",
    "(wstar*100).round(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AU    -0.0\n",
       "CA    40.3\n",
       "FR    -5.5\n",
       "DE    18.7\n",
       "JP     0.0\n",
       "UK   -13.2\n",
       "US   -40.3\n",
       "dtype: float64"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w_eq  = w_msr(delta*sigma_prior, pi, scale=False)\n",
    "# Display the difference in Posterior and Prior weights\n",
    "np.round(wstar - w_eq/(1+tau), 3)*100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>AU</th>\n",
       "      <th>CA</th>\n",
       "      <th>FR</th>\n",
       "      <th>DE</th>\n",
       "      <th>JP</th>\n",
       "      <th>UK</th>\n",
       "      <th>US</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>AU</th>\n",
       "      <td>1.000</td>\n",
       "      <td>0.488</td>\n",
       "      <td>0.478</td>\n",
       "      <td>0.515</td>\n",
       "      <td>0.439</td>\n",
       "      <td>0.512</td>\n",
       "      <td>0.491</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CA</th>\n",
       "      <td>0.488</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.664</td>\n",
       "      <td>0.655</td>\n",
       "      <td>0.310</td>\n",
       "      <td>0.608</td>\n",
       "      <td>0.779</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>FR</th>\n",
       "      <td>0.478</td>\n",
       "      <td>0.664</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.861</td>\n",
       "      <td>0.355</td>\n",
       "      <td>0.783</td>\n",
       "      <td>0.668</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DE</th>\n",
       "      <td>0.515</td>\n",
       "      <td>0.655</td>\n",
       "      <td>0.861</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.354</td>\n",
       "      <td>0.777</td>\n",
       "      <td>0.653</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>JP</th>\n",
       "      <td>0.439</td>\n",
       "      <td>0.310</td>\n",
       "      <td>0.355</td>\n",
       "      <td>0.354</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.405</td>\n",
       "      <td>0.306</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>UK</th>\n",
       "      <td>0.512</td>\n",
       "      <td>0.608</td>\n",
       "      <td>0.783</td>\n",
       "      <td>0.777</td>\n",
       "      <td>0.405</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.652</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>US</th>\n",
       "      <td>0.491</td>\n",
       "      <td>0.779</td>\n",
       "      <td>0.668</td>\n",
       "      <td>0.653</td>\n",
       "      <td>0.306</td>\n",
       "      <td>0.652</td>\n",
       "      <td>1.000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       AU     CA     FR     DE     JP     UK     US\n",
       "AU  1.000  0.488  0.478  0.515  0.439  0.512  0.491\n",
       "CA  0.488  1.000  0.664  0.655  0.310  0.608  0.779\n",
       "FR  0.478  0.664  1.000  0.861  0.355  0.783  0.668\n",
       "DE  0.515  0.655  0.861  1.000  0.354  0.777  0.653\n",
       "JP  0.439  0.310  0.355  0.354  1.000  0.405  0.306\n",
       "UK  0.512  0.608  0.783  0.777  0.405  1.000  0.652\n",
       "US  0.491  0.779  0.668  0.653  0.306  0.652  1.000"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rho"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
